{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embarradita de Natural Language Processing\n",
    "\n",
    "- Bag Of Words\n",
    "- Word Embeddings\n",
    "- Word2Vec\n",
    "- RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\"The quick brown fox jumps over the lazy dog\",\n",
    "             \"never jump over the lazy dog quickly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog never jump over the lazy dog quickly'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(sentences)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brown': 0,\n",
       " 'dog': 1,\n",
       " 'fox': 2,\n",
       " 'jump': 3,\n",
       " 'jumps': 4,\n",
       " 'lazy': 5,\n",
       " 'never': 6,\n",
       " 'over': 7,\n",
       " 'quick': 8,\n",
       " 'quickly': 9,\n",
       " 'the': 10}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = re.split(r\"\\ \", text)\n",
    "words = list(map(lambda x: x.lower(), words))\n",
    "uniques = np.unique(words)\n",
    "corpus = {w: i for i,w in enumerate(uniques)}\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 2],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_words(phrase, word):\n",
    "    lowercase = lambda x: x.lower()\n",
    "    return lowercase(phrase).count(word)\n",
    "matr_sentence = []\n",
    "for sentence in sentences:\n",
    "    vector_sentence = list(map(lambda x: count_words(sentence, x),\\\n",
    "        uniques))\n",
    "    matr_sentence.append(vector_sentence)\n",
    "np.array(matr_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# $$ W : words \\rightarrow R_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### J. R. Firth 1957\n",
    "- \"You shall know a word by the company it keeps\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-43-45.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[king] - [man] + [woman] ~ = [queen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find most similar\n",
    "X = vector(biggest) - vector(big) + vector(small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### More examples for semantic relationships \n",
    "Efficient Estimation of Word Representations in Vector Space\n",
    " - https://arxiv.org/pdf/1301.3781.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Folder for BIN files for FastText: wiki.simple.bin & wiki.simple.vec\n",
    "# You can download them from: https://github.com/facebookresearch/fastText\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "model = FastText.load_fasttext_format('./BIN/wiki.simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " 300\n",
      "2 \n",
      " True\n",
      "3 \n",
      " [('brainstem', 0.7807184457778931), ('bloodâ€“brain', 0.7781210541725159), ('brains', 0.7710486650466919), ('midbrain', 0.760841429233551), ('brainy', 0.7459008693695068), ('forebrain', 0.7418298721313477), ('hindbrain', 0.7350934743881226), ('braint', 0.7265611886978149), ('braine', 0.6931329965591431), ('brained', 0.6897463798522949)]\n",
      "4 \n",
      " 0.42670387\n",
      "5 \n",
      " [('queen', 0.5129120349884033), ('kingship', 0.4836469888687134), ('kingz', 0.4800509810447693), ('kings', 0.4712149500846863), ('adulyadej', 0.46895354986190796), ('regnant', 0.4502074420452118), ('kingkong', 0.44608789682388306), ('womans', 0.4455949664115906), ('noblewoman', 0.4404858648777008), ('bhumibol', 0.4334842562675476)]\n"
     ]
    }
   ],
   "source": [
    "# Give the embedding of a given word\n",
    "print(1, \"\\n\", len(model.wv['brain']))\n",
    "# Test if a word is in the model\n",
    "print(2, \"\\n\", 'brain' in model.wv.vocab)\n",
    "# Give the most similar words\n",
    "print(3, \"\\n\", model.most_similar('brain'))\n",
    "# Compute similarity between two words\n",
    "print(4, \"\\n\", model.similarity('brain', 'synapse'))\n",
    "# Compute cosine distance between two groups of words\n",
    "model.n_similarity(['sushi', 'shop'],\\\n",
    "    ['japanese', 'restaurant'])\n",
    "# Make arithmetic with words\n",
    "print(5, \"\\n\", model.wv.most_similar(positive=['king', 'woman'],\n",
    "                            negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "R(W(\"The\"), W(\"quick\"), W(\"brown\"), W(\"fox\"), ...) = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 300)\n"
     ]
    }
   ],
   "source": [
    "wvects = list(map(lambda x: model.wv[x], sentences[0].split(\" \")))\n",
    "wvects = np.array(wvects)\n",
    "print(wvects.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems of distributed representation\n",
    "- Similarity and relatedness are not the same:\n",
    "    - example: \"man\" and \"male\" are similar,\n",
    "    - example: \"keyboard\" and \"computer\" are related but dissimilar\n",
    "- Word ambiguity, mexican spanish as example :_( :\n",
    "    - Multiple embeddings based on supervised disambiguation, Trask et all:  https://arxiv.org/abs/1511.06388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Commonly used pre-trained word embeddings (2016)\n",
    "- Word2Vec  https://code.google.com/archive/p/word2vec/,\n",
    "- multilingual Word2Vec  htttps://github.com/Kyubyong/wordvectors\n",
    "- Glove  http://nlp.stanford.edu/projects/glove/\n",
    "- Fastext  https://github.com/icoxfog417/fastTextJapaneseTutorial\n",
    "- LaxVec  https://github.com/alexandres/lexvec\n",
    "- Meta-Embeddings  http://cistern.cis.lmu.de/meta-emb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-43-56.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic idea of Word2Vec\n",
    "\n",
    "- Continuous Bag of words: starts from source context words does aggregation and transformation and predicts the target word.\n",
    "- Skip Gram model: has each target word  as input and predicts the context/surrounding\n",
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-03.png\">\n",
    "\n",
    "Weight matrix W with V rows (amount of words in vocabulary), and N columns (N nodes of hidden layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example, for train the Word2Vec model with tensorflow\n",
    "\n",
    "https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why a conventional Neural Network can not performs well for NLP?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-12.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-21.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-27.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But, what is a RNN?\n",
    "\n",
    "https://explained.ai/rnn/index.html\n",
    "\n",
    "- Basically, each layer in a RNN vectorizes data, the layer h (for hidden) does this.\n",
    "- What exactly is h (sometimes called s) in the recurrence relation representing an RNN: (leaving off the nonlinearity)? The variable name h is typically used because it represents the hidden state of the RNN. An RNN takes a variable-length input record of symbols (e.g., stock price sequence, document, sentence, or word) and generates a fixed-length vector in high dimensional space, called an embedding, that somehow meaningfully represents or encodes the input record. The vector is only associated with a single input record and is only meaningful in the context of a classification or regression problem; the RNN is just a component of a surrounding model. For example, the h vector is often passed through a final linear layer V (multiclass logistic regressor) to get model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN architectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-33.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-40.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-44-56.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-45-02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem with training an RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-45-11.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-45-20.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"Pictures/Screenshot from 2020-08-22 16-45-26.png\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
